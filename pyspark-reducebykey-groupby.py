# cevheri
# 2020

# NYC Open Data
# Fire Incident Dispatch Data
# The Fire Incident Dispatch Data file contains data that is generated by the
# Starfire Computer Aided Dispatch System. The data spans from the time
# the incident is created in the system to the time the incident is closed in the system.
# It covers information about the incident as it relates to the assignment
# of resources and the Fire Departmentâ€™s response to the emergency.
# To protect personal identifying information in accordance with the Health Insurance Portability
# and Accountability Act (HIPAA), specific locations of incidents are not included
# and have been aggregated to a higher level of detail.


from pyspark import SparkConf, SparkContext

import sys
import os

os.environ['SPARK_HOME'] = "/home/cevher/apps/spark-3.0.1-bin-hadoop2.7"
# os.environ['HADOOP_HOME'] = "/home/cevher/app/spark-3.0.1-bin-hadoop2.7/hadoop"
sys.path.append("/home/cevher/apps/spark-3.0.1-bin-hadoop2.7/python")
sys.path.append("/home/cevher/apps/spark-3.0.1-bin-hadoop2.7/python/lib")

if __name__ == '__main__':
    conf = SparkConf().setMaster("local").setAppName("totals_by_count")
    sc = SparkContext(conf=conf)


    def parse_line(line):
        fields = line.split(",")
        in_date = fields[2]
        in_class = fields[16]
        return (in_date, in_class)


    lines = sc.textFile("Fire_Incident_Dispatch_Data.csv").flatMap(lambda line: line.split(",")) .map(lambda word: (word, 1)).reduceByKey(lambda (x, y): (x + y))

    # rdd = lines.map(parse_line)
    rdd.map(lambda word: (word, 1)).groupByKey().map(lambda (x, y): (x, sum(y)))

    rdd2 = rdd.groupBy("in_date", "in_class")
    # rdd2.show()

    totals_by_count = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x, y))
    totals_by_count.show()
